# Develop solutions that use Cosmos DB storage
what is AZ Cosmos DB ?
- AZ Cosmos DB is a globally distributed database system that allows to read and write data from the local replicas of your database and it transparently replicates the data to all the regions associated with your Cosmos account.

What are the benefits?
- Low latency
- High availability
- Elastic scalability of throughput
- Global distribution with 99.999% read and write availability all around the world.

There are five types of databases supported within cosmos DB. 
1. Core document DB(SQL) Recommended 
2. For Mongo DB
3. Cassandra
4. Azure table within Cosmos DB
5. Gremlin (Graph): Deals with nodes and edges

## AZ Cosmos Resource hierarchy
![AZ Cosmos resource hierarchy](AZCosmoshierarchy.PNG)
<h3>Exploring the hierarchy</h3>
Azure cosmos DB account has a unique DNS name and you can control via portal, CLI, SDK. You can virtually have an unlimited provisioned throughput (RU/s) and storage on a container. note: Max 50 Cosmos accounts under an Azure subscription. On request can be increased. 
<h4>Azure Cosmos DB containers</h4>



 Depending on the API the terms can be called as 

 SQL API | Database | Container|Item|
 ---------|----------|---------|---------|
 Mongo DB | Database | Collection|Document|
 Gremling | Database | Graph|Node/Edge|
 Cassandra|keyspace|Table|Row|
 |Table|NA|Table|Item|

 <h4>Azure Cosmos DB Partitions</h4>

`Logical Partitions:` Items in a container are divided into subsets called logical partitions.
`Partition Keys` The partition for the item is decided by the partition key that is associated with the item in the container
`ItemId` Each item also has an item id. To uniquely identify an item in the partition.

<h4>Request Units</h4>
- The cost of database operations is measured in terms of Request Units.
- Request Units is a blended measure of CPU, IOPS and mempory
- Requests Units are used no matter which API you are using for the Azure Cosmos DB account.
- The cost of reading a single item (1kb) is 1 Request Unit.
<h3>Capacitiy Mode</h3>  
While creating the Azure cosmos DB account you will select either of Provisioned Throughput or Serverless model in the capacity mode input.

<h4>Provisioned Throughput</h4>
- Provision the no of Request units at the container or database level.
- You are billed on a Hourly basis. You can always increment or decrement the no of request units provisioned.
<h4>Serverless Mode</h4> 
- No throughput provision is request. 
- Its managed by Azure Cosmos DB. you will be billed on the RU you consume.
<h4>Autoscale Mode</h4> 
- RU can automatically scale based on demand checked both at the container and database level. Great for mission critical workloads.

Sample Azure Cosmos Acount URL `https://dev-norwayeast-parcelhub-ci-cosmodbaccount.documents.azure.com:443/`

## Exploring consistency level

Developers can use these consistency options to make precise choices and granular tradeoffs with respect to high availability and performance

Stronger consistency                                                    Weaker consistency
- strong - bounded staleness- session(default)- consistent prefix- eventual
![Alt text](AZ%20Cosmos%20Availability%20and%20throughput%20level.png)


- `Strong` ensures the write operation happens to all replica immediately. 
- `Bounded Staleness` Allows to set the maximum quantity of time (or the version count) a replica document can be behind the primary document.
- `Session` This guarantees that all read and write operations are consistent within 
a session. The session can be shared by multiple app instance accessing multiple regions when they use the same session token all of the application instance will see the same reads and writes, application not in this session will be in Eventual consistency levels.
- `Consistent Prefix` allows for delay in the replication, but ensures that write in all replica showup in the correct/same order.
- `Eventual` will write the data in all replica, but doesn't guarantee the order of the writing. It consumes the lowest RU

**Strong:** A Strong consistency is basically guaranteed that when a region writes a piece of data that all of the other replicated regions are able to read that data at the exact same moment.This means that reads are guaranteed to see the most recent write.
    -the staleness window is equivalent to zero, and the clients are guaranteed to read the latest committed value of the write operation.
**Bounded staleness:** Frequetly used in global distribution application. There is a little bit of delay in terms of the data being available in other regions.
  -  clients always read the value of a previous write, with a lag bounded by the staleness window.
**Session**: what it allows is that depending on the session of the user, that person's going to get consistency.So in Session A, they can write here, and then read in another region, and they're going to have consistency cause they're reading from the same session. But there is an undefined delay for different client reading same data in different region with in its session
**consistent prefix:** basically going to be all is displayed in the right order.But we can see that there is a definite lag between the time it's written, and when any particular region has it available for writing,
but there is a guarantee that the data updates will always come in the right order. It's going to be A, A-B, A-B-C, but it'll never be B-A-C, in terms of updates. Everything's guaranteed in the order,
**Eventual** No guarantee of the order (e.g) Microsoft gives the example of retweets, likes, comments that are not in thread fashion.
 - the staleness window is largely dependent on your workload. For example, if there are no write operations on the database, a read operation with eventual, session, or consistent prefix consistency levels is likely to yield the same results as a read operation with strong consistency level.
 - 
**The eventual consistency level offers the greatest throughput at the cost of weaker consistency.**

## Choosing the right consistency
## Set the appropriate consistency level for operations
- Most Real world scenario use session
- Applications requires strong consistency then bounded staleness
- If you need highest availability, and the lowest latency, then eventual consistence 
- And then use  bounded staleness for stricter consistency and single digit milli sec latency for writing, 
- And then use consistent prefix for less strict consistency than the one provided by session consistency


### SQL API and Table API

## Exploring the supported APIS 

**SQL API**
- Stores data in document format. 
- Support querying items using Structured Query Language(SQL) 
- Most suitable for migrating from other databases oracle, Dyamo DB
**Mongo DB**
- Stores data in document structure, via BJSON format.
- Uses broader Mongo DB eco systems, without comprismisng Azure Cosmos DB features like scaling, geo-repl, high availabilty. 
- Existing Mongo DB app using the Mongo DB API
**Cassandra**
- Stores data in column-orientation schema.
- Cassandra API helps to intract with data using CQL
**Table API**
- Stores data in key/value format.
- Table API overcomes the limitation in using Azure table storage like latency, throughput, low query performance.
**Gremline API**
- API allows users to make graph queries and stores data as edges and vertices 
- Use this API for dynamic data, data with complex relationships, data that is  too complex to be designed with relational databases.

## Exploring request unit RU
- In Azure cosmos DB you have to provision request unit (RU) unit and storage that you consumer on hourly basis.
- The cost of all database operations is normalized by RU units.
- The cost to do a point read, which is fetching a single item by its ID and partition key value, for a 1KB item is 1RU.
-  A request unit represents the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.
-  Capacity mode types 
   -  Provisioned throughput mode. RU units are Provisioned at hourly basis 
   -  Serverless mode. No RU units provisioned. Billings are calculated at the end of the month
   -  Autoscale mode. Auto scaling the RU units 

# Implementing partitions in Azure Cosmos DB.
- Azure cosmos DB uses paritioning to scale individual container in a database to meet the performance 
  needs of your application
- Types of paritioning
  - **logical partition:** It is formed based on the value of a paritions key that is associated with each item in a container.
    - In addition to the parition key the items with the container has a unique id called ItemId. Combining the partiontion id and Item Id forms the index to identify a unique item within a logical partition  
    - (e.g) A container contains the food nutrition, all items contain a `foodGroup` property you can use `foodGroup` as the parition key for the container. Groups of items  that have specific values
    for `foodGroup` such as `Beef Products`, `Baked products` and Sausages form the logical paritions.
  - **Physical Partition**
    - Typically smaller containers contains one or more logical paritions those logical paritions are mapped to a single physical parition internally in Azure Cosmos DB
    - Each individial partitions can provide a throughput of up to 10,000 RU/secs.  The 10,000 RU/s limit for physical partitions implies that logical partitions also have a 10,000 RU/s limit, as each logical partition is only mapped to one physical partition.
    - The total data storage **(each individual physical partition can store up to 50GB data**).
  
## Choosing a parition key
  - A partition key has two components
    - partition key path,  accepts alphanumeric and _ values
    - partition key value,  can be string or numeric  
-  For example, consider an item { "userId" : "Andrew", "worksFor": "Microsoft" } if you choose “userId” as the partition key, the following are the two partition key components:
-  "/userId" is the partition key and Andrew is parition value.
-  Once a partition key is in place it is not possible to change it.

## Partition keys for read-heavy container.
- For a ready-heavy container choose a partition key that is frequently used in the queries. Queries can be effeciently routed to only the relevant physical partitions by including the partition key in the filter predicate.
-  If your container could grow to more than a few physical partitions, then you should make sure you pick a partition key that minimizes cross-partition queries.

```
Your container will require more than a few physical partitions when either of the following are true:

Your container will have over 30,000 RU's provisioned

Your container will store over 100 GB of data
```
**Using item ID as the partition key**
 For small read-heavy containers or write-heavy containers of any size, the item ID is naturally a great choice for the partition key.

 ## Things to consider using ItemID as the parition keys
 1. The Item ID does a great job at evenly balancing the RU consumption and data storage.
 2. Efficient point reading since you know an item's partition key if you know its item ID.
 3. It becomes the a unique identifier throughout your entire container.
 4. If you read heavy container that has a lot of physical partitions. Queries will be more efficient if they have an equality filter with an item ID.
 5. Can't run stored procedures or triggers across multiple logical partitions.

## Creating a synthetic partition key
- Concatinating multple property values into a single artificial `PartitionKey` property.
 ```
  {
  "deviceId": "abc-123",
  "date": 2018,
  "partitionKey": "abc-123-2018"
  }
 ```
 ## Use a partition key with precalculated suffixes
- The random suffix strategy can greatly improve write throughput, but it's difficult to read a specific item.
- Now suppose that each item has a Vehicle-Identification-Number (VIN) attribute that we want to access. Further, suppose that you often run queries to find items by the VIN, in addition to date. Before your application writes the item to the container, it can calculate a hash suffix based on the VIN and append it to the partition key date. 

With this strategy, the writes are evenly spread across the partition key values, and across the partitions. You can easily read a particular item and date, because you can calculate the partition key value for a specific Vehicle-Identification-Number. The benefit of this method is that you can avoid creating a single hot partition key, i.e., a partition key that takes all the workload.

### Coding 
https://docs.microsoft.com/en-us/azure/developer/java/spring-framework/how-to-guides-spring-data-cosmosdb

### Others: 
<h3>Composite Index</h3>
When you want to do order by multiple column. you need to create a composite index on those columns in the containers -> Settings->Indexing policy
`select * from c.orders order by c.category, c.quantity`
<h3> Time to live (TTL)</h3>
Here the items from the container can be deleted automatically after a certain period of time.
The deletion process uses the left over RU to delete the items in the container that has TTL expired.
The deletion process is managed by Azure cosmos DB automatically.
containers -> Settings -> TTL

# Reference:

[Change Feed Processor](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/change-feed-processor?tabs=dotnet)

[Cosmos Consistency](https://learn.microsoft.com/en-us/azure/cosmos-db/consistency-levels#configure-the-default-consistency-level)
**Using item ID as the partition key**
 For small read-heavy containers or write-heavy containers of any size, the item ID is naturally a great choice for the partition key.

 ## Things to consider using ItemID as the partition keys
 1. The Item ID does a great job at evenly balancing the RU consumption and data storage.
 2. Efficient point reading since you know an item's partition key if you know its item ID.
 3. It becomes a unique identifier throughout your entire container.
 4. If you read heavy container that has a lot of physical partitions. Queries will be more efficient if they have an equality filter with an item ID.
 5. Can't run stored procedures or triggers across multiple logical partitions.

## Creating a synthetic partition key
- Concatenating multiple property values into a single artificial `PartitionKey` property.
 ```
  {
  "deviceId": "abc-123",
  "date": 2018,
  "partitionKey": "abc-123-2018"
  }
 ```

## Use a partition key with precalculated suffixes
- The random suffix strategy can greatly improve write throughput, but it's difficult to read a specific item.
- Now suppose that each item has a Vehicle-Identification-Number (VIN) attribute that we want to access. Further, suppose that you often run queries to find items by the VIN, in addition to date. Before your application writes the item to the container, it can calculate a hash suffix based on the VIN and append it to the partition key date. 

With this strategy, the writes are evenly spread across the partition key values, and across the partitions. You can easily read a particular item and date, because you can calculate the partition key value for a specific Vehicle-Identification-Number. The benefit of this method is that you can avoid creating a single hot partition key, i.e., a partition key that takes all the workload.

## Perform operations on containers and items by using the SDK

   1. CosmosClient client = new CosmosClient(endpoint, key);
   2. Database database = await client.CreateDatabaseIfNotExistsAsync(databaseId, 10000);
   3. Database readResponse = await database.ReadAsync();
   4. await database.DeleteAsync();
   5. Creating a container
        ```
          1. // Set throughput to the minimum value of 400 RU/s
        ContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(
            id: containerId,
            partitionKeyPath: partitionKey,
            throughput: 400);
        ```
  1. Reading a container.      
```
      Container container = database.GetContainer(containerId);
      ContainerProperties containerProperties = await container.ReadContainerAsync();
```
  1. await database.GetContainer(containerId).DeleteContainerAsync();
  2. ItemResponse<SalesOrder> response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));
  3. Reading an item
      ```
      string id = "[id]";
      string accountNumber = "[partition-key]";
      ItemResponse<SalesOrder> response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));
      ```
  4.  Querying an item
   QueryDefinition query = new QueryDefinition(
    "select * from sales s where s.AccountNumber = @AccountInput ")
    .WithParameter("@AccountInput", "Account1");
    ```
    FeedIterator<SalesOrder> resultSet = container.GetItemQueryIterator<SalesOrder>(
        query,
        requestOptions: new QueryRequestOptions()
        {
            PartitionKey = new PartitionKey("Account1"),
            MaxItemCount = 1
        });    
    ```

## Implement change feed notifications
Change feed in Cosmos DB
- `Record changes` Here changes to the container are recorded in the order that they occur.
- `Feed` The feed consists of inserts, updates and not deletes
- `Process change feed` You can process the change feed with the use of AZ functions or a change feed processor.
- `Records` The change feed records are stored in a separate container
- `Sorting` The change feed is sorted in the order of the modification within each logical partition key value.
- `Throughput` The same provisioned throughput can be use to read the container containing the change feed.
- `Scaling` The persisted changes can be processed asynchronously and incrementally, and the output can be distributed across one or more consumers for parallel processing.

### Components of the change feed

`The monitored container:` The monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.

`The lease container:` The lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.

`The compute instance:` A compute instance hosts the change feed processor to listen for changes. Depending on the platform, it could be represented by a VM, a kubernetes pod, an Azure App Service instance, or an actual physical machine. It has a unique identifier referenced as the instance name throughout this article.

`The delegate:` The delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.


Ways to Implement the Change feed notifications. 
1. Create an Azure function of type Azure Cosmos DB triggers. 
   
2. Create a custom change feed processor using the Azure SDK
 1. Create cosmosClient, database, monitor container and leaser container. 
 2. startChangeFeedProcessorAsync instance. 
   ```
    Container leaseContainer = cosmosClient.GetContainer(databaseName, leaseContainerName);
    ChangeFeedProcessor changeFeedProcessor = cosmosClient.GetContainer(databaseName, sourceContainerName)
        .GetChangeFeedProcessorBuilder<ToDoItem>(processorName: "changeFeedSample", onChangesDelegate: HandleChangesAsync)
            .WithInstanceName("consoleHost")
            .WithLeaseContainer(leaseContainer)
            .Build();

    Console.WriteLine("Starting Change Feed Processor...");
    await changeFeedProcessor.StartAsync();
   ```
 3. HandleChangeFeed Async
```
   static async Task HandleChangesAsync(
    ChangeFeedProcessorContext context,
    IReadOnlyCollection<ToDoItem> changes,
    CancellationToken cancellationToken)
{
    Console.WriteLine($"Started handling changes for lease {context.LeaseToken}...");
    Console.WriteLine($"Change Feed request consumed {context.Headers.RequestCharge} RU.");
    // SessionToken if needed to enforce Session consistency on another client instance
    Console.WriteLine($"SessionToken ${context.Headers.Session}");

    // We may want to track any operation's Diagnostics that took longer than some threshold
    if (context.Diagnostics.GetClientElapsedTime() > TimeSpan.FromSeconds(1))
    {
        Console.WriteLine($"Change Feed request took longer than expected. Diagnostics:" + context.Diagnostics.ToString());
    }

    foreach (ToDoItem item in changes)
    {
        Console.WriteLine($"Detected operation for item with id {item.id}, created at {item.creationTime}.");
        // Simulate some asynchronous operation
        await Task.Delay(10);
    }

    Console.WriteLine("Finished handling changes.");
}

```

## Coding 
https://docs.microsoft.com/en-us/azure/developer/java/spring-framework/how-to-guides-spring-data-cosmosdb
