# Develop solutions that use Cosmos DB storage
what is AZ Cosmos DB ?
- AZ Cosmos DB is a globally distributed database system that allows to read and write data from the local replicas of your database and it transparently replicates the data to all the regions associated with your Cosmos account.

What are the benefits?
- Low latency
- High availability
- Elastic scalability of throughput
- Global distribution with 99.999% read and write availability all around the world.

There are five types of databases supported within cosmos DB. 
1. Core document DB(SQL) Recommended 
2. For Mongo DB
3. Cassandra
4. Azure table within Cosmos DB
5. Gremlin (Graph): Deals with nodes and edges

## AZ Cosmos Resource hierarchy
![AZ Cosmos resource hierarchy](AZCosmoshierarchy.PNG)
- Max 50 Cosmos accounts under an Azure subscription. On request can be increased.
- Unlimited throughput(RU/s) and storage on a container


 Depending on the API the Azure cosmos database and Cosmos items can be called as below

|Azure Cosmos entity|SQL API|Cassandra API|Azure Cosmos DB API for MongoDB|Gremlin API|Table API|
|---|---|---|---|---|---|
|Azure Cosmos database|Database|Keyspace|Database|Database|NA|
|Azure Cosmos item|Item|Row|Document|Node or edge|Item|


Sample Azure Cosmos Acount URL `https://dev-norwayeast-parcelhub-ci-cosmodbaccount.documents.azure.com:443/`

## Exploring consistency level

Developers can use these consistency options to make precise choices and granular tradeoffs with respect to high availability and performance

Stronger consistency                                                    Weaker consistency
- strong - bounded staleness- session(default)- consistent prefix- eventual

A Strong consistency is basically guaranteed that when a region writes a piece of data that all of the other replicated regions are able to read that data at the exact same moment.

**Strong:** This means that reads are guaranteed to see the most recent write.
    -the staleness window is equivalent to zero, and the clients are guaranteed to read the latest committed value of the write operation.
**Bounded staleness:** Frequetly used in global distribution application. There is a little bit of delay in terms of the data being available in other regions.
  -  clients always read the value of a previous write, with a lag bounded by the staleness window.
**Session**: what it allows is that depending on the session of the user, that person's going to get consistency.So in Session A, they can write here, and then read in another region, and they're going to have consistency cause they're reading from the same session. But there is an undefined delay for different client reading same data in different region with in its session
**consistent prefix:** basically going to be all is displayed in the right order.But we can see that there is a definite lag between the time it's written, and when any particular region has it available for writing,
but there is a guarantee that the data updates will always come in the right order. It's going to be A, A-B, A-B-C, but it'll never be B-A-C, in terms of updates. Everything's guaranteed in the order,
**Eventual** No guarantee of the order (e.g) Microsoft gives the example of retweets, likes, comments that are not in thread fashion.
 - the staleness window is largely dependent on your workload. For example, if there are no write operations on the database, a read operation with eventual, session, or consistent prefix consistency levels is likely to yield the same results as a read operation with strong consistency level.
 - 
The eventual consistency level offers the greatest throughput at the cost of weaker consistency.

## Choosing the right consistency
### SQL API and Table API
- Most Real world scenario use session
- Applications requires strong consistency then bounded staleness
- If you need highest availability, and the lowest latency, then eventual consistence 
- And then use  bounded staleness for stricter consistency and single digit milli sec latency for writing, 
- And then use consistent prefix for less strict consistency than the one provided by session consistency

## Exploring the supported APIS 

**SQL API**
- Stores data in document format. 
- Support querying items using Structured Query Language(SQL) 
- Most suitable for migrating from other databases oracle, Dyamo DB
**Mongo DB**
- Stores data in document structure, via BJSON format.
- Uses broader Mongo DB eco systems, without comprismisng Azure Cosmos DB features like scaling, geo-repl, high availabilty. 
- Existing Mongo DB app using the Mongo DB API
**Cassandra**
- Stores data in column-orientation schema.
- Cassandra API helps to intract with data using CQL
**Table API**
- Stores data in key/value format.
- Table API overcomes the limitation in using Azure table storage like latency, throughput, low query performance.
**Gremline API**
- API allows users to make graph queries and stores data as edges and vertices 
- Use this API for dynamic data, data with complex relationships, data that is  too complex to be designed with relational databases.

## Exploring request unit RU
- In Azure cosmos DB you have to provision request unit (RU) unit and storage that you consumer on hourly basis.
- The cost of all database operations is normalized by RU units.
- The cost to do a point read, which is fetching a single item by its ID and partition key value, for a 1KB item is 1RU.
-  A request unit represents the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.
-  Capacity mode types 
   -  Provisioned throughput mode. RU units are Provisioned at hourly basis 
   -  Serverless mode. No RU units provisioned. Billings are calculated at the end of the month
   -  Autoscale mode. Auto scaling the RU units 

# Implementing partitions in Azure Cosmos DB.
- Azure cosmos DB uses paritioning to scale individual container in a database to meet the performance 
  needs of your application
- Types of paritioning
  - **logical partition:** It is formed based on the value of a paritions key that is associated with each item in a container.
    - In addition to the parition key the items with the container has a unique id called ItemId. Combining the partiontion id and Item Id forms the index to identify a unique item within a logical partition  
    - (e.g) A container contains the food nutrition, all items contain a `foodGroup` property you can use `foodGroup` as the parition key for the container. Groups of items  that have specific values
    for `foodGroup` such as `Beef Products`, `Baked products` and Sausages form the logical paritions.
  - **Physical Partition**
    - Typically smaller containers contains one or more logical paritions those logical paritions are mapped to a single physical parition internally in Azure Cosmos DB
    - Each individial partitions can provide a throughput of up to 10,000 RU/secs.  The 10,000 RU/s limit for physical partitions implies that logical partitions also have a 10,000 RU/s limit, as each logical partition is only mapped to one physical partition.
    - The total data storage **(each individual physical partition can store up to 50GB data**).
  
## Choosing a parition key
  - A partition key has two components
    - partition key path,  accepts alphanumeric and _ values
    - partition key value,  can be string or numeric  
-  For example, consider an item { "userId" : "Andrew", "worksFor": "Microsoft" } if you choose “userId” as the partition key, the following are the two partition key components:
-  "/userId" is the partition key and Andrew is parition value.
-  Once a partition key is in place it is not possible to change it.

## Partition keys for read-heavy container.
- For a ready-heavy container choose a partition key that is frequently used in the queries. Queries can be effeciently routed to only the relevant physical partitions by including the partition key in the filter predicate.
-  If your container could grow to more than a few physical partitions, then you should make sure you pick a partition key that minimizes cross-partition queries.

```
Your container will require more than a few physical partitions when either of the following are true:

Your container will have over 30,000 RU's provisioned

Your container will store over 100 GB of data
```
**Using item ID as the partition key**
 For small read-heavy containers or write-heavy containers of any size, the item ID is naturally a great choice for the partition key.

 ## Things to consider using ItemID as the partition keys
 1. The Item ID does a great job at evenly balancing the RU consumption and data storage.
 2. Efficient point reading since you know an item's partition key if you know its item ID.
 3. It becomes a unique identifier throughout your entire container.
 4. If you read heavy container that has a lot of physical partitions. Queries will be more efficient if they have an equality filter with an item ID.
 5. Can't run stored procedures or triggers across multiple logical partitions.

## Creating a synthetic partition key
- Concatenating multiple property values into a single artificial `PartitionKey` property.
 ```
  {
  "deviceId": "abc-123",
  "date": 2018,
  "partitionKey": "abc-123-2018"
  }
 ```

## Use a partition key with precalculated suffixes
- The random suffix strategy can greatly improve write throughput, but it's difficult to read a specific item.
- Now suppose that each item has a Vehicle-Identification-Number (VIN) attribute that we want to access. Further, suppose that you often run queries to find items by the VIN, in addition to date. Before your application writes the item to the container, it can calculate a hash suffix based on the VIN and append it to the partition key date. 

With this strategy, the writes are evenly spread across the partition key values, and across the partitions. You can easily read a particular item and date, because you can calculate the partition key value for a specific Vehicle-Identification-Number. The benefit of this method is that you can avoid creating a single hot partition key, i.e., a partition key that takes all the workload.

## Perform operations on containers and items by using the SDK

   1. CosmosClient client = new CosmosClient(endpoint, key);
   2. Database database = await client.CreateDatabaseIfNotExistsAsync(databaseId, 10000);
   3. Database readResponse = await database.ReadAsync();
   4. await database.DeleteAsync();
   5. Creating a container
        ```
          1. // Set throughput to the minimum value of 400 RU/s
        ContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(
            id: containerId,
            partitionKeyPath: partitionKey,
            throughput: 400);
        ```
  1. Reading a container.      
```
      Container container = database.GetContainer(containerId);
      ContainerProperties containerProperties = await container.ReadContainerAsync();
```
  1. await database.GetContainer(containerId).DeleteContainerAsync();
  2. ItemResponse<SalesOrder> response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));
  3. Reading an item
      ```
      string id = "[id]";
      string accountNumber = "[partition-key]";
      ItemResponse<SalesOrder> response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));
      ```
  4.  Querying an item
   QueryDefinition query = new QueryDefinition(
    "select * from sales s where s.AccountNumber = @AccountInput ")
    .WithParameter("@AccountInput", "Account1");
    ```
    FeedIterator<SalesOrder> resultSet = container.GetItemQueryIterator<SalesOrder>(
        query,
        requestOptions: new QueryRequestOptions()
        {
            PartitionKey = new PartitionKey("Account1"),
            MaxItemCount = 1
        });    
    ```

## Implement change feed notifications
Change feed in Cosmos DB
- `Record changes` Here changes to the container are recorded in the order that they occur.
- `Feed` The feed consists of inserts, updates and not deletes
- `Process change feed` You can process the change feed with the use of AZ functions or a change feed processor.
- `Records` The change feed records are stored in a separate container
- `Sorting` The change feed is sorted in the order of the modification within each logical partition key value.
- `Throughput` The same provisioned throughput can be use to read the container containing the change feed.
- `Scaling` The persisted changes can be processed asynchronously and incrementally, and the output can be distributed across one or more consumers for parallel processing.

### Components of the change feed

`The monitored container:` The monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.

`The lease container:` The lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.

`The compute instance:` A compute instance hosts the change feed processor to listen for changes. Depending on the platform, it could be represented by a VM, a kubernetes pod, an Azure App Service instance, or an actual physical machine. It has a unique identifier referenced as the instance name throughout this article.

`The delegate:` The delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.


Ways to Implement the Change feed notifications. 
1. Create an Azure function of type Azure Cosmos DB triggers. 
   
2. Create a custom change feed processor using the Azure SDK
 1. Create cosmosClient, database, monitor container and leaser container. 
 2. startChangeFeedProcessorAsync instance. 
   ```
    Container leaseContainer = cosmosClient.GetContainer(databaseName, leaseContainerName);
    ChangeFeedProcessor changeFeedProcessor = cosmosClient.GetContainer(databaseName, sourceContainerName)
        .GetChangeFeedProcessorBuilder<ToDoItem>(processorName: "changeFeedSample", onChangesDelegate: HandleChangesAsync)
            .WithInstanceName("consoleHost")
            .WithLeaseContainer(leaseContainer)
            .Build();

    Console.WriteLine("Starting Change Feed Processor...");
    await changeFeedProcessor.StartAsync();
   ```
 3. HandleChangeFeed Async
```
   static async Task HandleChangesAsync(
    ChangeFeedProcessorContext context,
    IReadOnlyCollection<ToDoItem> changes,
    CancellationToken cancellationToken)
{
    Console.WriteLine($"Started handling changes for lease {context.LeaseToken}...");
    Console.WriteLine($"Change Feed request consumed {context.Headers.RequestCharge} RU.");
    // SessionToken if needed to enforce Session consistency on another client instance
    Console.WriteLine($"SessionToken ${context.Headers.Session}");

    // We may want to track any operation's Diagnostics that took longer than some threshold
    if (context.Diagnostics.GetClientElapsedTime() > TimeSpan.FromSeconds(1))
    {
        Console.WriteLine($"Change Feed request took longer than expected. Diagnostics:" + context.Diagnostics.ToString());
    }

    foreach (ToDoItem item in changes)
    {
        Console.WriteLine($"Detected operation for item with id {item.id}, created at {item.creationTime}.");
        // Simulate some asynchronous operation
        await Task.Delay(10);
    }

    Console.WriteLine("Finished handling changes.");
}

```

## Coding 
https://docs.microsoft.com/en-us/azure/developer/java/spring-framework/how-to-guides-spring-data-cosmosdb
