
# Develop solutions that use blob storage

what is Azure Blob storage? 
- Microsoft's object storage solution for the cloud
- For storing massive amount of unstructured data like text or binary data.
- Designed for Storing files for distributed access, Streaming audio and video, data for backup and restore, disaster recovery, and archiving.
- Accessable via HTTP/HTTPS. REST API, Azure PowerShell, Azure CLI, or an Azure Storage client library
- Azure storage account is the top container for all of the Azure Blob storage. It provides a unique namespace for Azure storage data that is accessible from anywhere in the world over HTTP or HTTPS.

## Types of storage account.
- **Standard** general-purpose v2 account. Recommended for most scenarios using Azure storage.
- **Premium** High performance by using solid-state drive. If you create a premium account you can choose between three account types: **_block blobs, page blobs, or file shares_**.

|Storage account type |     Supported storage services| Detail|
|-----|------|------|
|Standard general-purpose v2 |Blob, Queue, and Table storage, Azure Files| This is standard storage for blobs, file shares, queues and tables.|
|Premium block blobs|Blob storage|This is supported for block and appen blobs. This is when you want fast access to your blobs, high transaction rates.|
|Premium page blobs|Page blobs only|When you want fast access to your blobs, high transaction rates.|
|Premium file shares|Azure Files|When you want fast access to your files, high transaction rates.|

## Access tiers for block blob data

- **Hot**: Frequently accessed data, Accessing data is cheap, but storing costs are higher
- **Cool**: Large amounts of data that is infrequently accesses and stored for atleast 30 days.
- **Archive**: Available only for individual Block blobs.For atleast 180 days.Its the most cheapest access tier for storage, but accessing the data is more expensive.
> New storage accounts are created in the hot tier by default.

## Evaluate Azure Storage redundancy options
Data in an Azure Storage account is always replicated three times in the primary region.
|Redundancy||
|---|---|
|**Locally redundant storage (LRS)**| copies three times within a single physical locations in the primary region.|
| | Least expensive replication option|
||Not suitable for high availability or durability|
|**Zone-redundant storage (ZRS)**| Copies three times  across three Azure availability zones in the primary region.| 
||High availability|
|**Geo-redundant storage (GRS)**| 3 copies in the single physical locations in the primary region using LRS and then 3 copies in the single physical locations in the secondary region using LRS |
|**Geo-zone-redundant storage (GZRS)** | 3 Copies across three Azure availability zones in the primary region using ZRS and then in the secondary region using ZRS|

Note: GRS and GZRS are the secondary region redundancy options

<h3>Blob services</h3>
The blob service is optimized for storing large amounts of unstructured data. So if you want to store files such as your images, your videos, etc., you can post it using the blob service.

<p>You can then have an application that could be hosted on an as the virtual machine accessing these objects that are stored using the blob service in your Azure storage account.
 Now when you start using the blob service as part of your storage account, you create something known has a container.So this is like having a folder allowed folder for your objects.
 </p>

You will create a container and then within this container you will start uploading objects such as your files, images, your videos, etc..

Now each of these is going to be uploaded, has an object, a binary object onto the service, and you

also get a unique you order for each object that is part of the blob service.

Once you upload an object to the container you get a unique Blob storage endpoint like:
 `http://mystorageaccount.blob.core.windows.net/data/AZ-204.jpg`
Note: mystorageaccount - Its the storage account name, blob.core.windows.net - service name, data - container name, and AZ-204.jpg is the object stored in the container.


## Resource types
**Storage account**, **container** in the storage account, **A blob** in the container
> A storage account can contain unlimited containers and a container can contain unlimited blobs within it.

**Azure supports three types of Blobs:**
- **Block blobs**  store text and binary data, up to 190.7 TB. It is made up of blocks of data that can be managed individually.s
- **Append blobs** similar to block blobs optimized for append operations.Ideal for logging data from VM.
- **Page blobs** store random access files up to 8 TB. Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.

### Azure Storage security features
- All data (including metadata) written to Azure Storage is automatically encrypted using Storage Service Encryption (SSE).
-  Azure Active Directory (Azure AD) is supported for blob and queue data operations 
- Use AD for Resource management operations such as key management.
-  Role-Based Access Control (RBAC) You can assign RBAC roles  scoped to a subscription, resource group, storage account, or an individual container or queue to a security principal or a managed identity for Azure resources.
-  Data can be secured in transit between the application and Azure using 
  Client-side Encryption, HTTPS, or SMB 3.0
- OS, VM, and data disk can be encrypted using Azure Disk Encryption.
- Delegated access to the Azure storage can be granted using Shared access signature.
-  `Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption
-  Encryption is enabled by default for Azure storage accounts and cannot be disabled.
-  Encryption are default regardless of the tier (Standard or Premium) or deployment model (Azure Resource Manager or classic)
-  There is no extra cost for the default encryption.

### Encryption key management
- **A customer-managed key** is used to encrypt and decrypt all data in all services in your storage account.
- **A customer-provided key** on Blob storage operations. A client making a read or write request against Blob storage can include an encryption key on the request for granular control over how blob data is encrypted and decrypted.

# Implement static site hosting
Static content (HTML, CSS, JavaScript, and image files) can be stored in a storage container named $web.<br>
Limitation: No custom headers configuration supported.<br>
How to enable it? AZ portal -> Storage account -> Static website -> Enabled -> Configure index.html and error page 404.html.<br>
Modifying `the public access level of the $web container from Private (no anonymous access) to Blob (anonymous read access for blobs only)`, then the level of public access to the primary static website endpoint https://contosoblobaccount.z22.web.core.windows.net/index.html doesn't change.

However, `the public access to the primary blob service endpoint` https://contosoblobaccount.blob.core.windows.net/$web/index.html does change from private to public. Now users can open that file by using either of these two endpoints.

Disabling public access on a storage account by using the public access setting of the storage account doesn't affect static websites that are hosted in that storage account.

# Implement storage policies, and data archiving and retention


## Blob Access policy
The access tier can be set while uploading a blob or after uploading.
|Tier| Details|
|---|---|
|Hot and Cold| Access tier at both blob and Account level|
|Archive| Only blob level|
|Hot and Cold| Support all redundant options|
|Archive|Supports only LRS, GRS, and RA-GRS.|
- Data storage limits are set at the account level and  not per access tier(Hot, Cold, Archive).

`Data stored in a premium block blob storage account cannot be tiered to Hot, Cool, or Archive using Set Blob Tier or using Azure Blob Storage lifecycle management.`
 To move data, you must synchronously copy blobs from the block blob storage account to the Hot tier in a different account using the Put Block From URL API or a version of AzCopy that supports this API. 
 The Put Block From URL API synchronously copies data on the server, meaning the call completes only once all the data is moved from the original server location to the destination location.

## Manage the data lifecycle
- Life cycle management policy rules are available to move aging data to cooler tiers.
- Delete blobs at the end of their lifecycles.
- 
### Implement storage policies, and data archiving and retention

A life cycle management policy is a collection of rules in a JSON document. Each rule definition within a policy includes a filter set and an action set. 
The filter set limits rule actions to a certain set of objects within a container or object names. The action set applies the tier or deletes actions to the filtered set of objects.

```json 
{
  "rules": [
    {
      "name": "ruleFoo",
      "enabled": true,
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": [ "blockBlob" ],
          "prefixMatch": [ "container1/foo" ]
        },
        "actions": {
          "baseBlob": {
            "tierToCool": { "daysAfterModificationGreaterThan": 30 },
            "tierToArchive": { "daysAfterModificationGreaterThan": 90 },
            "delete": { "daysAfterModificationGreaterThan": 2555 }
          },
          "snapshot": {
            "delete": { "daysAfterCreationGreaterThan": 90 }
          }
        }
      }
    }
  ]
}
```

<h4>Azure CLI</h4>
To add a lifecycle management policy with Azure CLI, write the policy to a JSON file

```shell
az storage account management-policy create \
    --account-name <storage-account> \
    --policy @policy.json \
    --resource-group <resource-group>
```
## Rule actions
- Actions are applied to the filtered blobs when the run condition is met.
- Some sample actions: `tierToCool`, `enableAutoTierToHotFromCool`, `tierToArchive` and `delete`.
- `delete` is cheaper than action `tierToArchive`. Action `tierToArchive` is cheaper than action `tierToCool`.
- The run conditions are based on age. `Base blobs use the last modified time to track age`, and `blob snapshots use the snapshot creation time to track age`.
- **Run condition:** daysAfterModificationGRaterThan, daysAfterCreationGreaterThan.


|Action|	Base Blob	|Snapshot	|Version|
|---|---|---|---|
|tierToCool	|Supported for blockBlob|	Supported|	Supported|
|enableAutoTierToHotFromCool|	Supported for blockBlob|	Not supported|	Not supported|
|tierToArchive|	Supported for blockBlob	|Supported|	Supported|
|delete|	Supported for blockBlob and appendBlob|	Supported	|Supported|


## Rehydrate blob data from the archive tier.
There are two options for rehydrating a blob that is stored in the archive tier.
- **Copy an archived blob to an new blob in the hot or cool tier** with the Copy Blob or Copy Blob from URL operation
  - Copying an archived blob to an online destination tier is supported within the same storage account only.
- **Change a blob's access tier to an online tier**  rehydrate an archived blob to hot or cool by changing its tier using the `Set Blob Tier` operation.

### Rehydration Priority
 using optional header `x-ms-rehydrate-priority` the rehydration operation can be properitized 
- **Standard priority:**  request is processed in the order it was received and may take up to 15 hours
- **High priority:**: Rehyrdation prioritized over standard prioritzed and done in an hour for objects under 10 GB in size.

To check the rehydration priority while the rehydration operation is underway, call Get Blob Properties to return the value of the `x-ms-rehydrate-priority` header. The rehydration priority property returns either Standard or High.

Azure Blob storage lifecycle management offers a rich, rule-based policy for General Purpose v2 and Blob storage accounts.
 
## Perform operations on data by using the appropriate SDK

How to perform operations using the Azure Blob storage client library. Find the class purpose 
- `BlobClient` - Allows to manipulate AZ Storage-specific blob, incl operations: upload, download, delete, and create snapshots.
- `AppendBlobClient` - provides operation specific to append blobs, such as appending log data.
- `BlobClientOptions` - provides operations specific to block blobs, such as staging and then committing blocks of data.
- `BlobContainerClient` - `Allows manipulating AZ storage containers and their blobs.`
- `BlobServiceClient` - Allows to manipulate of AZ Storage service resources and blob containers. The storage account provides the top-level namespace for the Blob service.
- `BlobUriBuilder` Provides a convenient way to modify the contents of a URI instance to point to diff AZ storage resources like an account, container, or blob

```js
using Azure.Storage.Blobs; //primary classes (client objects)
using Azure.Storage.Blobs.Models; //All other utility classes, structures, and enum
Azure.Storage.Blobs.Specialized; // Contains classes that you can use to perform operations specific to a blob type, such as block blobs.

//1. creating BlobServiceClient

 BlobServiceClient client = new(
        new Uri($"https://{accountName}.blob.core.windows.net"),
        new DefaultAzureCredential());

//2. Create the container client using the service client object
  BlobContainerClient client = blobServiceClient.GetBlobContainerClient(containerName);

// Append the container name to the end of the URI, to work specific single container, step1 ignored
  BlobContainerClient client = new(
      new Uri($"https://{accountName}.blob.core.windows.net/{containerName}"),
      new DefaultAzureCredential(),
      clientOptions);

//3 create a BlobClient
 BlobClient client =
        blobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(blobName);

// Create the container and return a container client object
BlobContainerClient containerClient = await blobServiceClient.CreateBlobContainerAsync(containerName);

//Upload blobs to the container 

// Open the file and upload its data
using (FileStream uploadFileStream = File.OpenRead(localFilePath)) {
  await blobClient.UploadAsync(uploadFileStream);
  uploadFileStream.Close();
}

//List blobs
await foreach (BlobItem blobItem in containerClient.GetBlobsAsync())
{
    Console.WriteLine("\t" + blobItem.Name);
}

// Download the blob's contents and save it to a file
BlobDownloadInfo download = await blobClient.DownloadAsync();

using (FileStream downloadFileStream = File.OpenWrite(downloadFilePath))
{
    await download.Content.CopyToAsync(downloadFileStream);
}

// Delete the container and clean up local files created
Console.WriteLine("\n\nDeleting blob container...");
await containerClient.DeleteAsync();

```
### Set and retrieve properties, metadata from the BlobContainerClient

<h4>Retrieve contain properties</h4>

`GetProperties`, `GetPropertiesAsync`

```java
// Fetch some container properties and write out their values.
        var properties = await container.GetPropertiesAsync();
        Console.WriteLine($"Properties for container {container.Uri}");
        Console.WriteLine($"Public access level: {properties.Value.PublicAccess}");
        Console.WriteLine($"Last modified time in UTC: {properties.Value.LastModified}");
```
<h4>set and Retrieve metadata</h4>

Please note metadata headers are name/value pairs. (e.g)`x-ms-meta-name:string-value`

Set Metadata: `await container.SetMetadataAsync(metadata);`

Get Metadata:

```java
 var properties = await container.GetPropertiesAsync();

        // Enumerate the container's metadata.
        Console.WriteLine("Container metadata:");
        foreach (var metadataItem in properties.Value.Metadata)
        {
            Console.WriteLine($"\tKey: {metadataItem.Key}");
            Console.WriteLine($"\tValue: {metadataItem.Value}");
        }
```        

### Set and retrieve properties and metadata for blob resources by using REST

 GET and HEAD operations both retrieve metadata headers for the specified container or blob

 <h3>Returns the Metadata from the container.</h3>
 
 `GET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container`

 `GET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata`


<h3>Setting Metadata Headers</h3>
 The PUT operation sets metadata headers on the specified container or blob, overwriting any existing metadata on the resource. Calling PUT without any headers on the request clears all existing metadata on the resource.

Put on the container level.
 `PUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&restype=container`

Put on the blob level.
 `PUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata`

 <h3>Standard HTTP properties for container and blobs </h3> 

The standard HTTP headers supported on containers include:

`ETag`, `Last-Modified`

The standard HTTP headers supported on blobs include:

`ETag`, `Last-Modified`, Content-Length, Content-Type, Content-MD5, Content-Encoding, Content-Language, Cache-Control, Origin, Range
